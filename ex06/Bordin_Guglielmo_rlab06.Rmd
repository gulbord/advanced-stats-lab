---
title: R Laboratory – Exercise 6
author: Guglielmo Bordin
date: "`r gsub('^0', '', format(Sys.Date(), '%d %B %Y'))`"
output:
    prettydoc::html_pretty:
        theme: cayman
        highlight: github
        math: katex
---

```{r setup, include = FALSE}
library(tidyverse)
library(kableExtra)
theme_set(theme_minimal(base_size = 14, base_family = "Open Sans"))
my_pal <- RColorBrewer::brewer.pal(3, "Dark2")

# knitr setup
knitr::opts_chunk$set(
    dev = "svg", fig.width = 8, fig.height = 5,
    message = FALSE, warning = FALSE
)

set.seed(20230603)
```

# Hand-made Markov Chain
We are given the following un-normalized posterior distribution

$$
    g(\theta \; \mid \; x) \sim \frac{1}{2}
        \exp\biggl[-\frac{(\theta + 3)^2}{2}\biggr] + \frac{1}{2}
        \exp\biggl[-\frac{(\theta - 3)^2}{2}\biggr],
$$

and we want to draw a Markov Chain from it, using a Metropolis-Hastings
algorithm.

Let’s define the functions in R. We’ll choose a gaussian proposal
distribution, with the standard deviation given as input from the user.

```{r}
posterior <- function(t) {
    0.5 * exp(-0.5 * (t + 3)^2) + 0.5 * exp(-0.5 * (t - 3)^2)
}

metropolis <- function(target, proposal_std, niter, burn_in, thinning) {
    chain <- numeric(niter)
    x_old <- 0 # trial move
    accepted <- 0

    for (i in seq_along(chain)) {
        # generate trial move
        x_new <- rnorm(1, x_old, proposal_std)

        # accept with metropolis test
        acc_prob <- min(1, target(x_new) / target(x_old))
        if (runif(1) < acc_prob) {
            x_old <- x_new
            accepted <- accepted + 1
        }

        # store result
        chain[i] <- x_old
    }

    # discard burn-in and thin
    chain <- chain[seq(burn_in, length(chain), by = thinning)]

    list(chain = chain, acc_rate = accepted / niter)
}
```

Let’s test the algorithm and display the results in a plot.

```{r}
std <- 1
niter <- 1e5
burn_in <- 1000
thinning <- 10

mc <- metropolis(posterior, std, niter, burn_in, thinning)

cat("Acceptance rate =", mc$acc_rate)
```

```{r}
true_func <- tibble(
    theta = seq(-8, 8, by = 0.01),
    # we've to normalize the posterior here
    post = posterior(theta) / integrate(posterior, -8, 8)$value
)

tibble(chain = mc$chain) |>
    ggplot(aes(x = chain)) +
        geom_histogram(
            aes(y = after_stat(density)),
            binwidth = 0.2, fill = "#e6a452"
        ) +
        geom_line(
            aes(x = theta, y = post),
            data = true_func,
            colour = "#945a10", linewidth = 0.8
        ) +
        labs(x = "θ", y = "Posterior g(θ | x)")
```

The agreement is good. Now, let’s analyse the chain with the `CODA` package
and plot the chain autocorrelation.

```{r}
library(coda)

chain <- as.mcmc(mc$chain)

tibble(lag = seq(0, 500, by = 10), acor = autocorr(chain, lags = lag)) |>
    ggplot(aes(lag, acor)) +
        geom_hline(yintercept = 0, colour = "gray40", linetype = 2) +
        geom_point(size = 2, colour = "#7b7bb4") +
        geom_line(linewidth = 0.8, colour = "#7b7bb4") +
        labs(x = "Lag", y = "ACF")
```

Let’s experiment with changing the burn-in cycle’s size, to see if there’s
perhaps a better value than the one we have chosen initially, 1000.
To analyse the performance, we’ll monitor both the acceptance rate and the
effective size returned by `CODA`.

```{r}
purrr::map(
    c(500, 750, 1000, 1500, 2000, 4000, 8000),
    function(b) {
        res <- metropolis(posterior, std, niter, b, thinning)
        tibble(
            burn_in = b,
            acc_rate = res$acc_rate,
            eff_size = effectiveSize(res$chain)
        )
    }
) |> list_rbind() |> kbl() |> kable_styling(full_width = FALSE)
```

The effective size is quite low with every choice of `burn_in`, and doesn’t
change much. We’ll stick with the initial choice of 1000 for simplicity.

We can also try different values for the proposal distribution’s standard
deviation and the thinning interval. The acceptance rate will only be
affected by the standard deviation value, so we can check it separately,
keeping the previous thinning value of 10.

```{r}
purrr::map(
    c(0.5, 0.75, 1, 1.25, 1.5),
    function(s) {
        res <- metropolis(posterior, s, niter, burn_in, thinning)
        tibble(
            std = s,
            acc_rate = res$acc_rate,
            eff_size = effectiveSize(res$chain)
        )
    }
) |> list_rbind() |> kbl() |> kable_styling(full_width = FALSE)
```

There’s a clear trade-off between the effective size and the acceptance
rate: a wider proposal distribution leads to more uncorrelated samples, but
it’s also “riskier”, lowering the acceptance rate. Let’s analyse the
effective size to choose the best parameters.

```{r}
get_eff_size <- function(s, t) {
    effectiveSize(metropolis(posterior, s, niter, burn_in, t)$chain)
}

expand_grid(
    std = c(0.5, 0.75, 1, 1.25, 1.5),
    thinning = c(5, 10, 20, 40, 80)
) |>
    mutate(eff_size = purrr::map2_dbl(std, thinning, get_eff_size)) |>
    ggplot(aes(factor(std), factor(thinning))) +
        geom_tile(aes(fill = eff_size)) +
        scale_fill_viridis_c(name = "Effective size") +
        labs(x = "Proposal standard deviation", y = "Thinning factor")
```

We can see that the thinning interval isn’t affecting the effective size as
much as the standard deviation of the proposal distribution.  Probably the
most sensible thing to do, given these results, is to raise the standard
deviation a bit, to 1.25 or 1.5. The other parameters seem good enough with
the chosen values.

# COVID-19 charts
We’ll analyse the global vaccination dataset provided by “Our World in
Data”, available at ```https://ourworldindata.org/covid-vaccinations```.

```{r}
data <- read_csv("owid-covid-data.csv")
```

The CSV file follows a format of one row per location and date, keeping
track of many variables:

```{r}
spec(data)
```

The first thing that jumps out from a quick analysis of the `iso_code` and
`location` columns is that, among all the countries and territories, there
are also some “grouping” rows, e.g. low income or high income countries, or
continents.

```{r}
data |>
    select(iso_code, location) |>
    filter(str_starts(iso_code, "OWID")) |>
    distinct()
```

So, we could directly work with the `OWID_WRL` rows, and get the
whole-world data. We’ll later make sure that it checks out if we take the
longer route of putting together all the rows pertaining to real single
locations.

We’ll start with the vaccination data: let’s make a quick plot of the
cumulative number of people who received at least one vaccine dose, per
hundred (we find this in the `people_vaccinated_per_hundred` column).

```{r}
data |>
    filter(location == "World") |>
    select(date, people_vaccinated_per_hundred) |>
    drop_na() |>
    ggplot(aes(date, people_vaccinated_per_hundred)) +
        geom_area(
            linewidth = 0.8, alpha = 0.5,
            colour = my_pal[1], fill = my_pal[1]
        ) +
        xlab("Date") +
        ylab("Vaccinated population (%)")
```

Let’s see if we get the same plot by grouping the locations.

```{r}
data_no_owid <- data |> filter(!str_starts(iso_code, "OWID"))

data_no_owid |>
    drop_na(people_vaccinated, population) |>
    group_by(date) |>
    summarize(pvac = 100 * sum(people_vaccinated) / sum(population)) |>
    ggplot(aes(date, pvac)) +
        geom_point(size = 0.8, alpha = 0.5) +
        geom_smooth(
            span = 0.25, linewidth = 0.8,
            colour = my_pal[2], fill = my_pal[2], alpha = 0.5
        ) +
        xlab("Date") +
        ylab("Vaccinated population (%)")
```

As we can see, the trend is indeed similar. We can’t say much about the
discrepancies, not knowing how the `OWID_WRL` data is constructed.

Let’s examine now the distribution of new doses administered by day,
smoothed over a seven-day period. We can find this data in the column
`new_vaccinations_smoothed`.

```{r}
data_no_owid |>
    drop_na(new_vaccinations_smoothed, population) |>
    group_by(date) |>
    summarize(
        nvac = 100 * sum(new_vaccinations_smoothed) / sum(population)
    ) |>
    ggplot(aes(date, nvac)) +
        geom_area(
            linewidth = 0.8, alpha = 0.5,
            fill = my_pal[1], colour = my_pal[1]
        ) +
        xlab("Date") +
        ylab("New vaccinations (% of total population)")
```

We can also try a more interesting analysis by looking at the
“Low/Middle/High Income” rows in the dataset, to see how higher income
countries got hold of the vaccines sooner than the lower income ones. To
avoid the double peaks of the second doses, we’ll look at the variable
`new_people_vaccinated_smoothed`, which filters out the administrations of
second or successive doses.

```{r, fig.width = 10, fig.height = 6}
data |>
    filter(str_ends(location, "income")) |>
    drop_na(new_people_vaccinated_smoothed) |>
    select(date, location, vac = new_people_vaccinated_smoothed) |>
    mutate(location = fct_relevel(
        location,
        "High income",
        "Upper middle income",
        "Lower middle income",
        "Low income"
    )) |>
    ggplot(aes(date, vac)) +
        geom_area(
            aes(date, total),
            data = function(x) {
                x |> group_by(date) |>
                     summarize(total = sum(vac))
            }, colour = "grey40", fill = "grey80", alpha = 0.5
        ) +
        geom_area(
            aes(colour = location, fill = location),
            alpha = 0.5, position = "identity"
        ) +
        scale_y_continuous(
            labels = scales::label_number(suffix = "M", scale = 1e-6)
        ) +
        scale_colour_viridis_d(option = "plasma", guide = "none") +
        scale_fill_viridis_d(option = "plasma") +
        labs(
            x = "Date",
            y = "First doses (7-day smoothed)",
            fill = "Country income"
        )
```

Let’s look at the death count now. First, the cumulative total.

```{r}
data_no_owid |>
    drop_na(total_deaths) |>
    group_by(date) |>
    summarize(deaths = sum(total_deaths)) |>
    ggplot(aes(date, deaths)) +
        geom_area(
            linewidth = 0.8, alpha = 0.5,
            colour = my_pal[3], fill = my_pal[3]
        ) +
        scale_y_continuous(
            breaks = scales::pretty_breaks(),
            labels = scales::label_number(suffix = "M", scale = 1e-6),
            limits = c(0, 8e6)
        ) +
        labs(x = "Date", y = "Total confirmed deaths")
```

And then, the weekly average (from the column `new_deaths_smoothed`).

```{r}
data_no_owid |>
    drop_na(new_deaths_smoothed) |>
    group_by(date) |>
    summarize(deaths = sum(new_deaths_smoothed)) |>
    ggplot(aes(date, deaths)) +
        geom_area(
            linewidth = 0.8, alpha = 0.5,
            colour = my_pal[3], fill = my_pal[3]
        ) +
        scale_y_continuous(
            breaks = scales::pretty_breaks(),
            labels = scales::label_number(suffix = "k", scale = 1e-3)
        ) +
        labs(x = "Date", y = "Daily confirmed deaths (7-day smoothed)")
```