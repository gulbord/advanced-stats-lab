---
title: R Laboratory – Exercise 4
author: Guglielmo Bordin
date: "`r gsub('^0', '', format(Sys.Date(), '%d %B %Y'))`"
output:
    prettydoc::html_pretty:
        theme: architect
        highlight: github
        math: katex
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
    dev = "svg", fig.width = 8, fig.height = 5, message = FALSE
)
```

# Insurance claims
The number of claims received by an insurance company during a week follows
a Poisson distribution with unknown mean $\mu$. The claims per week
observed over a ten week period are:
```
5, 8, 4, 6, 11, 6, 6, 5, 6, 4
```

We’ll start by computing the posterior distribution with the assumption of
a uniform prior for $\mu$. Let’s first define a helpful `bayes` function
to calculate everything we need in the analysis.
```{r}
bayes <- function(lhood, prior, max, cred_int = c(0.025, 0.975)) {
    dx <- 0.001
    x <- seq(0, max, by = dx)
    # get denominator (evidence)
    evid <- integrate(\(y) lhood(y) * prior(y), 0, max)$value
    # assemble posterior
    post <- lhood(x) * prior(x) / evid
    cdf <- cumsum(post) * dx

    # get posterior mode, median, mean and std
    mode <- x[which.max(post)]
    median <- x[which.max(cdf > 0.5)]
    mean <- sum(x * post) * dx
    std <- sqrt(sum((x - mean)^2 * post) * dx)

    # get credibility interval
    cred <- sapply(cred_int, \(c) x[which.max(cdf > c)])

    # return named list
    list(
        post = post, mode = mode, med = median,
        mean = mean, std = std, cred = cred
    )
}
```
Now, let’s start with the uniform prior.
```{r}
claims <- c(5, 8, 4, 6, 11, 6, 6, 5, 6, 4)
n <- length(claims)

# poissonian likelihood
lhood <- \(m) m^sum(claims) * exp(-n * m)

max_mu <- 1.5 * max(claims)
dmu <- 0.001
# g(mu) = 1 for all mu --> improper prior (divergent integral)
post_unif <- bayes(lhood, \(m) 1, max = max_mu)
```
```{r, echo = FALSE}
library(tidyverse)

tibble(
    post_unif$mode, post_unif$med, post_unif$mean, post_unif$std,
    paste(post_unif$cred, collapse = ", ")
) |>
    set_names("Mode", "Median", "Mean", "St. dev.", "95% cred. int.") |>
    kableExtra::kbl(caption = "Posterior parameters") |>
    kableExtra::kable_styling(full_width = FALSE)
```
Now, we’ll use a Jeffrey’s prior instead, that is

$$
    g(\mu) = \frac{1}{\sqrt{\mu}}.
$$

This is another improper prior, but it’s not a problem since we normalize
numerically the posterior inside the `bayes` function.

To handle the value at 0 correctly, we also set $g(\mu) = 0$ for $\mu = 0$,
to avoid a divergence in the posterior.
```{r}
post_jeff <- bayes(lhood, \(m) ifelse(m > 0, 1 / sqrt(m), 0), max_mu)
```
```{r, echo = FALSE}
tibble(
    post_jeff$mode, post_jeff$med, post_jeff$mean, post_jeff$std,
    paste(post_jeff$cred, collapse = ", ")
) |>
    set_names("Mode", "Median", "Mean", "St. dev.", "95% cred. int.") |>
    kableExtra::kbl(caption = "Posterior parameters") |>
    kableExtra::kable_styling(full_width = FALSE)
```

Let’s plot the two posteriors with their credibility intervals.
```{r}
my_pal <- wesanderson::wes_palette("Zissou1", 5)[c(1, 3, 5)]
theme_set(theme_minimal(base_size = 14, base_family = "Open Sans"))

prior_names <- c(unif = "Uniform", jeff = "Jeffrey’s")
prior_cols <- my_pal[1:2] |> set_names("unif", "jeff")
```
```{r}
tibble(
    mu = seq(0, max_mu, along.with = post_unif$post),
    unif = post_unif$post,
    jeff = post_jeff$post,
) |>
    # transform to long format for ggplot
    pivot_longer(-1, names_to = "dist", values_to = "f") |>
    # reorder `dist` factors to follow the order in the text
    mutate(dist = fct_relevel(dist, "unif", "jeff")) |>
    ggplot(aes(x = mu, y = f, colour = dist)) +
        geom_line(linewidth = 0.8) +
        # shade 95% credibility intervals
        geom_area(
            aes(fill = dist),
            data = function(x) {
                x |> group_by(dist) |>
                     filter(mu > mu[which.max(cumsum(f) * dmu > 0.025)] &
                            mu < mu[which.max(cumsum(f) * dmu > 0.975)])
            },
            position = "identity",
            alpha = 0.5, show.legend = FALSE
        ) +
        scale_colour_manual(values = prior_cols, labels = prior_names) +
        scale_fill_manual(values = prior_cols) +
        labs(
            x = "Poisson parameter μ",
            y = "Posterior distribution P(μ | y)",
            colour = "Prior"
        ) +
        coord_cartesian(expand = FALSE, ylim = c(0, 0.55))
```

Finally, let’s approximate the two posteriors with a normal distribution
with the same mean and standard deviation, and compare the credibility
intervals.
```{r}
thr <- c(0.025, 0.975)

tibble(
    prior = c("Uniform", "Jeffrey’s"),
    mean = c(post_unif$mean, post_jeff$mean),
    std = c(post_unif$std, post_jeff$std),
    cred = c(
        paste(post_unif$cred, collapse = ", "),
        paste(post_jeff$cred, collapse = ", ")
    ),
    cred_n = c(
        paste(
            format(qnorm(thr, post_unif$mean, post_unif$std), digits = 4),
            collapse = ", "
        ),
        paste(
            format(qnorm(thr, post_jeff$mean, post_jeff$std), digits = 4),
            collapse = ", "
        )
    )
) |>
    mutate_if(is.numeric, format, digits = 4) |>
    set_names(
        "Prior", "Mean", "St. dev.",
        "95% cred. int.", "95% cred. int. (normal approx.)"
    ) |>
    kableExtra::kbl(caption = "Posterior parameters") |>
    kableExtra::kable_styling(full_width = FALSE)
```
The normal approximation works well enough in both cases; after all, with
10 data points this was to be expected.
```{r}
m <- seq(0, max_mu, along.with = post_unif$post)
type_cols <- my_pal[c(1, 3)]

tibble(
    mu = m,
    unif = post_unif$post,
    unif_n = dnorm(m, post_unif$mean, post_unif$std),
    jeff = post_jeff$post,
    jeff_n = dnorm(m, post_jeff$mean, post_jeff$std)
) |>
    pivot_longer(-mu, names_to = "dist", values_to = "f") |>
    # encode the original/approx. distinction in separate column
    mutate(
        type = ifelse(str_ends(dist, "_n"), "appr", "orig"),
        dist = str_remove(dist, "_n")
    ) |>
    # reorder factors as needed
    mutate(
        dist = fct_relevel(dist, "unif", "jeff"),
        type = fct_relevel(type, "orig", "appr")
    ) |>
    ggplot(aes(x = mu, y = f, colour = type)) +
        geom_line(linewidth = 0.8) +
        geom_area(
            aes(fill = type),
            data = function(x) {
                x |> group_by(dist, type) |>
                     filter(mu > mu[which.max(cumsum(f) * dmu > 0.025)] &
                            mu < mu[which.max(cumsum(f) * dmu > 0.975)])
            },
            position = "identity",
            alpha = 0.5, show.legend = FALSE
        ) +
        scale_colour_manual(
            values = type_cols, labels = c("Original", "Normal appr.")
        ) +
        scale_fill_manual(values = type_cols) +
        labs(
            x = "Poisson parameter μ",
            y = "Posterior distribution P(μ | y)",
            colour = "Type"
        ) +
        coord_cartesian(expand = FALSE, ylim = c(0, 0.55)) +
        facet_grid(
            rows = vars(dist),
            labeller = as_labeller(
                c(unif = "Uniform prior", jeff = "Jeffrey’s prior")
            )
        )
```

# Blood tests
A well established method for detecting a disease in blood fails to detect 
it in 15% of the patiants that actually have the disease.

A young UniPd start-up has developed an innovative method of screening.
During the qualification phase, a random sample of $n = 75$ patients known
to have the disease is screened using the new method.

The probability distribution of $y$, the number of times the new method
fails to detect the disease, is a binomial with $n = 75$. The method fails 
to detect the disease in 6 cases. Following a frequentist approach, we
would say that an unbiased estimator for the probability of our binomial
distribution is given by

$$
    \hat{p}_\mathrm{f} = \frac{y}{n} = \frac{6}{75} = 0.08,
$$

with standard deviation

$$
    \sigma = \sqrt{\operatorname{Var}[\hat{p}_\mathrm{f}]}
        = \sqrt{\frac{y}{n^2}\biggl(1 - \frac{y}{n}\biggr)}
        = 0.031.
$$

Let’s proceed now with a bayesian computation, assuming a beta prior with
mean value $0.15$ and standard deviation $0.14$.
```{r}
y <- 6
n <- 75

mu <- 0.15
sig <- 0.14

# formulas to get alpha and beta from mean and sigma
alpha <- mu * (mu * (1 - mu) / sig^2 - 1)
beta <- (1 - mu) * alpha / mu

post_beta <- bayes(
    lhood = \(p) p^y * (1 - p)^(n - y),
    # handle the 0 value correctly
    prior = \(p) ifelse(p > 0, dbeta(p, alpha, beta), 0),
    max = 1
)
```
Now we can plot the resulting posterior distribution, highlighting the mean
and an interval of one standard deviation around it.
```{r}
p <- seq(0, 1, along.with = post_beta$post)
tibble(p, fp = post_beta$post) |>
    ggplot(aes(x = p, y = fp)) +
        geom_line(
            aes(x = p, y = fp, linetype = "prior"),
            data = tibble(p, fp = dbeta(.data$p, alpha, beta)),
            linewidth = 0.8, colour = my_pal[1]
        ) +
        geom_line(
            aes(linetype = "post"),
            linewidth = 0.8, colour = my_pal[1]
        ) +
        # standard deviation shading
        geom_area(
            data = function(x) {
                m <- post_beta$mean
                s <- post_beta$std
                filter(x, p > m - s & p < m + s)
            },
            colour = my_pal[1], fill = my_pal[1],
            alpha = 0.5, show.legend = FALSE
        ) +
        # mean line
        geom_vline(xintercept = post_beta$mean, colour = my_pal[1]) +
        annotate(
            "label", x = 0.1, y = 13.5, hjust = 0,
            label = paste(
                "Mean ± Std =",
                format(post_beta$mean, digits = 2), "±",
                format(post_beta$std, digits = 2)
            )
        ) +
        scale_linetype_manual(
            values = c(post = "solid", prior = "dotted"),
            labels = c(post = "Posterior", prior = "Prior")
        ) +
        labs(
            x = "Binomial probability π",
            y = element_blank(),
            linetype = "Distribution"
        ) +
        coord_cartesian(expand = FALSE, ylim = c(0, 15))
```