---
title: R Laboratory – Exercise 3
author: Guglielmo Bordin
date: "`r gsub('^0', '', format(Sys.Date(), '%d %B %Y'))`"
output:
    rmdformats::readthedown:
        fig_width: 8
        fig_height: 5
        thumbnails: false
        lightbox: true
        gallery: true
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(dev = "svg", message = FALSE)
```

# Binomial inference
In this problem we’ll study the binomial inference for a study that reports
$y = 7$ successes in $n = 20$ independent trials.
```{r}
y <- 7  # number of successes
n <- 20 # number of trials
```
We’ll assume three different priors:

* a uniform distribution
* a Jeffrey’s prior
* a step function defined as
$$
    g(\pi) =
    \begin{cases}
        \pi       &       \pi \leqslant 0.2 \\
        0.2       & 0.2 < \pi \leqslant 0.3 \\
        0.5 - \pi & 0.3 < \pi \leqslant 0.5 \\
        0         & 0.5 < \pi
    \end{cases}
$$

Let’s implement the step function first…
```{r}
stepf <- function(x) {
   ifelse(x <= 0.2, x,
          ifelse(x <= 0.3, 0.2,
                 ifelse(x <= 0.5, 0.5 - x, 0)))
}
```
… and compute the three posterior distributions. For the uniform and
Jeffrey's prior, this is straightforward since they are just beta
distributions, which are the conjugate prior for the binomial likelihood.
Thus, the posteriors are beta distributions with different $\alpha$ and
$\beta$ values depending on the exponents resulting from the product of the
likelihood and the prior.
```{r}
alph_unif <- 1 + y
beta_unif <- 1 + n - y
alph_jeff <- 0.5 + y
beta_jeff <- 0.5 + n - y

post_unif <- function(p) dbeta(p, alph_unif, beta_unif)
post_jeff <- function(p) dbeta(p, alph_jeff, beta_jeff)
```
For the case of the step function, we need to multiply the likelihood and
the step prior, and then perform numerical integration to determine the
normalization constant.
```{r}
post_step <- function(p) {
    post_step_num <- \(x) stepf(x) * dbinom(y, n, x)
    post_step_num(p) / integrate(post_step_num, 0, 1)$value
}
```
Let’s plot the three posteriors on top of each other to compare them.
```{r}
library(tidyverse)
theme_set(theme_light(base_size = 14, base_family = "Lato"))

# probability vector
dp <- 1 / 2000
p <- seq(0, 1, by = dp)

# convert data to long format for ggplot
posts <- tibble(p, post_unif(p), post_jeff(p), post_step(p)) |>
    set_names("p", "unif", "jeff", "step") |>
    pivot_longer(-1, names_to = "dist", values_to = "fp") |>
    mutate(dist = fct_relevel(dist, "unif", "jeff", "step"))

prior_names <- c(unif = "Uniform", jeff = "Jeffrey’s", step = "Step")
# palette
my_pal <- c("#EE9C39", "#20A47B", "#5A3920")
prior_cols <- my_pal |> set_names("unif", "jeff", "step")

ggplot(posts, aes(x = p, y = fp, colour = dist)) +
    geom_line(linewidth = 0.8) +
    scale_colour_manual(values = prior_cols, labels = prior_names) +
    labs(x = "π", y = "g(π | y, n)", colour = "Prior") +
    coord_cartesian(expand = FALSE, ylim = c(0, 6))
```

To further the comparison, we can compute the first two moments of each 
posterior distribution.
```{r}
moment <- function(dist, n) {
    purrr::map_dbl(dist, \(f) integrate(\(p) p^n * f(p), 0, 1)$value)
}

means <- moment(c(post_unif, post_jeff, post_step), 1)
variances <- moment(c(post_unif, post_jeff, post_step), 2) - means^2
```

Now we’ll also compute a 95% credibility interval on each distribution.
For the uniform and Jeffrey’s it’s easy, we can use the built-in quantile
function `qbeta`. For the step function we have to find the probability
thresholds numerically.
```{r}
thresh <- c(0.025, 0.975)
creds <- list(
    unif = qbeta(thresh, alph_unif, beta_unif),
    jeff = qbeta(thresh, alph_jeff, beta_jeff),
    step = map_dbl(
        thresh,
        \(t) p[which(near(cumsum(post_step(p)) * dp, t, tol = dp / 2))]
    )
)
```

Let’s put the results in a summary table, and draw the limits on the
plot of the posterior distributions.
```{r}
summary_table <- function(prior_names, means, variances, creds) {
    tibble::tibble(prior_names, means, variances, creds) |>
        dplyr::mutate(creds = format(creds, digits = 3)) |>
        dplyr::mutate_if(is.numeric, format, digits = 3) |>
        rlang::set_names("Prior", "Mean", "Variance", "95% cred. int.") |>
        kableExtra::kbl() |>
        kableExtra::kable_styling(full_width = FALSE)
}

summary_table(prior_names, means, variances, creds)
```
```{r}
ggplot(posts, aes(x = p, y = fp, colour = dist)) +
    geom_line(linewidth = 0.8) +
    geom_ribbon(
        aes(ymin = 0, ymax = fp, fill = dist),
        data = posts |> group_by(dist) |>
            filter(p > creds[[unique(dist)]][1]
                   & p < creds[[unique(dist)]][2]),
        alpha = 0.5,
        show.legend = FALSE
    ) +
    scale_colour_manual(values = prior_cols, labels = prior_names) +
    scale_fill_manual(values = prior_cols) +
    labs(x = "π", y = "g(π | y, n)", colour = "Prior") +
    coord_cartesian(expand = FALSE, ylim = c(0, 6))
```

# Giardia cysts
Giardia cysts are a parasite that can contaminate food, water and surfaces,
and they can cause *giardiasis* when swallowed in this infective stage of
their life cycle. Infection occurs when a person swallows Giardia cysts 
from contaminated water, food, hands, surfaces or objects.

A group of researchers from a Human Health Department is working to
determine the quality of stream water. They collect a total of 116
one-liter water samples from sites suspected to have a heavy environmental
impact from birds and water flow. Out of these samples, 17 contain Giardia
cysts.

We’ll find the posterior distribution for the probability $\pi$ that one
sample contains Giardia cysts, assuming

* a uniform prior distribution
* a $\mathrm{Beta}(1, 4)$ prior

Once again, we’re looking at a binomial process, so the posterior
distributions will be just two other beta distributions, with different
$\alpha$ and $\beta$ depending on $y$ and $n$.

```{r}
y <- 17
n <- 116

alph_unif <- 1 + y
beta_unif <- 1 + n - y
alph_beta <- 1 + y
beta_beta <- 4 + n - y

post_unif <- function(p) dbeta(p, alph_unif, beta_unif)
post_beta <- function(p) dbeta(p, alph_beta, beta_beta)
```

Let’s compute mean, variance and 95% credibility intervals for the two
posteriors, and display the results in a summary table and a plot. We’ll
also add a normal approximation for the posterior, based on 
```{r}
means <- moment(c(post_unif, post_beta), 1)
variances <- moment(c(post_unif, post_beta), 2) - means^2

creds <- list(
    unif = qbeta(thresh, alph_unif, beta_unif),
    beta = qbeta(thresh, alph_beta, beta_beta)
)

prior_names <- c(unif = "Uniform", beta = "Beta(1, 4)")
prior_cols <- my_pal[1:2] |> set_names("unif", "beta")

summary_table(prior_names, means, variances, creds)

tibble(p, post_unif(p), post_beta(p)) |>
    set_names("p", "unif", "beta") |>
    pivot_longer(-1, names_to = "dist", values_to = "fp") |>
    mutate(dist = fct_relevel(dist, "unif", "beta")) |>
    ggplot(aes(x = p, y = fp, colour = dist)) +
        geom_line(linewidth = 0.8) +
        geom_ribbon(
            aes(ymin = 0, ymax = fp, fill = dist),
            data = function(x) {
                x |> group_by(dist) |>
                    filter(p > creds[[unique(dist)]][1]
                           & p < creds[[unique(dist)]][2])
            },
            alpha = 0.5,
            show.legend = FALSE
        ) +
        scale_colour_manual(values = prior_cols, labels = prior_names) +
        scale_fill_manual(values = prior_cols) +
        labs(x = "π", y = "g(π | y, n)", colour = "Prior") +
        coord_cartesian(expand = FALSE, ylim = c(0, 13.5)) +
        scale_y_continuous(breaks = scales::pretty_breaks())
```