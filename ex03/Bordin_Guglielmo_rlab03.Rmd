---
title: R Laboratory – Exercise 3
author: Guglielmo Bordin
date: "`r gsub('^0', '', format(Sys.Date(), '%d %B %Y'))`"
output:
    prettydoc::html_pretty:
        theme: cayman
        highlight: github
        math: katex
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
    dev = "svg", fig.width = 8, fig.height = 5, message = FALSE
)

set.seed(13)
```

# Binomial inference
In this problem we’ll study the binomial inference for a study that reports
$y = 7$ successes in $n = 20$ independent trials.
```{r}
y <- 7  # number of successes
n <- 20 # number of trials
```
We’ll assume three different priors:

* a uniform distribution
* a Jeffrey’s prior
* a step function defined as

$$
    g(\pi) =
    \begin{cases}
        \pi       &       \pi \leqslant 0.2 \\
        0.2       & 0.2 < \pi \leqslant 0.3 \\
        0.5 - \pi & 0.3 < \pi \leqslant 0.5 \\
        0         & 0.5 < \pi
    \end{cases}
$$

Let’s implement the step function first…
```{r}
stepf <- function(x) {
   ifelse(x <= 0.2, x,
          ifelse(x <= 0.3, 0.2,
                 ifelse(x <= 0.5, 0.5 - x, 0)))
}
```
… and compute the three posterior distributions. For the uniform and
Jeffrey's prior, this is straightforward since they are just beta
distributions, which are the conjugate prior for the binomial likelihood.
Thus, the posteriors are beta distributions with different $\alpha$ and
$\beta$ values depending on the exponents resulting from the product of the
likelihood and the prior.
```{r}
alph_unif <- 1 + y
beta_unif <- 1 + n - y
alph_jeff <- 0.5 + y
beta_jeff <- 0.5 + n - y

post_unif <- function(p) dbeta(p, alph_unif, beta_unif)
post_jeff <- function(p) dbeta(p, alph_jeff, beta_jeff)
```
For the case of the step function, we need to multiply the likelihood and
the step prior, and then perform numerical integration to determine the
normalization constant.
```{r}
post_step <- function(p) {
    post_step_num <- \(x) stepf(x) * dbinom(y, n, x)
    post_step_num(p) / integrate(post_step_num, 0, 1)$value
}
```
Let’s plot the three posteriors on top of each other to compare them.
```{r}
library(tidyverse)
theme_set(theme_light(base_size = 14, base_family = "Open Sans"))

# probability vector with step size dp
dp <- 1 / 2000
p <- seq(0, 1, by = dp)

posts <- tibble(
    p = p,
    unif = post_unif(p),
    jeff = post_jeff(p),
    step = post_step(p)
) |>
    # transform to long format for ggplot
    pivot_longer(-1, names_to = "dist", values_to = "fp") |>
    # reorder `dist` factors to follow the order in the text
    mutate(dist = fct_relevel(dist, "unif", "jeff", "step"))

prior_names <- c(unif = "Uniform", jeff = "Jeffrey’s", step = "Step")
# define color palette and map colours to prior names
my_pal <- c("#EE9C39", "#20A47B", "#5A3920")
prior_cols <- my_pal |> set_names("unif", "jeff", "step")

ggplot(posts, aes(x = p, y = fp, colour = dist)) +
    geom_line(linewidth = 0.8) +
    scale_colour_manual(values = prior_cols, labels = prior_names) +
    labs(x = "π", y = "g(π | y, n)", colour = "Prior") +
    # make plot start at (0, 0)
    coord_cartesian(expand = FALSE, ylim = c(0, 6))
```

To further the comparison, we can compute the first two moments of each 
posterior distribution.
```{r}
# define function for later use
moment <- function(dist, n) {
    # integrate each function in dist
    purrr::map_dbl(dist, \(f) integrate(\(p) p^n * f(p), 0, 1)$value)
}

means <- moment(c(post_unif, post_jeff, post_step), 1)
variances <- moment(c(post_unif, post_jeff, post_step), 2) - means^2
```

Now we’ll also compute a 95% credibility interval on each distribution.
For the uniform and Jeffrey’s it’s easy, we can use the built-in quantile
function `qbeta`. For the step function we have to find the probability
thresholds numerically.
```{r}
thresh <- c(0.025, 0.975)
creds <- list(
    unif = qbeta(thresh, alph_unif, beta_unif),
    jeff = qbeta(thresh, alph_jeff, beta_jeff),
    step = map_dbl(
        thresh,
        # compute the cdf and find the two p's close to 0.025, 0.975
        \(t) p[which(near(cumsum(post_step(p)) * dp, t, tol = dp / 2))]
    )
)
```

Let’s put the results in a summary table, and draw the limits on the
plot of the posterior distributions.
```{r}
# put table code into function for later use
summary_table <- function(prior_names, means, variances, creds) {
    tibble::tibble(prior_names, means, variances, creds) |>
        # format digits in every column
        dplyr::mutate(creds = format(creds, digits = 3)) |>
        dplyr::mutate_if(is.numeric, format, digits = 3) |>
        rlang::set_names("Prior", "Mean", "Variance", "95% cred. int.") |>
        kableExtra::kbl(caption = "Posterior parameters") |>
        kableExtra::kable_styling(full_width = FALSE)
}

summary_table(prior_names, means, variances, creds)
```
```{r}
ggplot(posts, aes(x = p, y = fp, colour = dist)) +
    geom_line(linewidth = 0.8) +
    # shade 95% credibility intervals
    geom_ribbon(
        aes(ymin = 0, ymax = fp, fill = dist),
        data = posts |>
            group_by(dist) |>
            # keep only the values inside the cred. int. for each dist
            filter(p > creds[[unique(dist)]][1] &
                   p < creds[[unique(dist)]][2]),
        alpha = 0.5,
        show.legend = FALSE
    ) +
    scale_colour_manual(values = prior_cols, labels = prior_names) +
    scale_fill_manual(values = prior_cols) +
    labs(x = "π", y = "g(π | y, n)", colour = "Prior") +
    coord_cartesian(expand = FALSE, ylim = c(0, 6))
```

# Giardia cysts
Giardia cysts are a parasite that can contaminate food, water and surfaces,
and they can cause giardiasis when swallowed in this infective stage of
their life cycle. Infection occurs when a person swallows Giardia cysts 
from contaminated water, food, hands, surfaces or objects.

A group of researchers from a Human Health Department is working to
determine the quality of stream water. They collect a total of 116
one-liter water samples from sites suspected to have a heavy environmental
impact from birds and water flow. Out of these samples, 17 contain Giardia
cysts.

We’ll find the posterior distribution for the probability $\pi$ that one
sample contains Giardia cysts, assuming

* a uniform prior distribution
* a $\operatorname{Beta}(1, 4)$ prior

Once again, we’re looking at a binomial process, so the posterior
distributions will be just two other beta distributions, with different
$\alpha$ and $\beta$ depending on $y$ and $n$.

```{r}
y <- 17
n <- 116

alph_unif <- 1 + y
beta_unif <- 1 + n - y
alph_beta <- 1 + y
beta_beta <- 4 + n - y

post_unif <- function(p) dbeta(p, alph_unif, beta_unif)
post_beta <- function(p) dbeta(p, alph_beta, beta_beta)
```

Let’s compute mean, variance and 95% credibility intervals for the two
posteriors, and display the results in a summary table and a plot. We’ll
also add a normal approximation for the posterior, based on 
```{r}
# use the function defined before on the new posteriors
means <- moment(c(post_unif, post_beta), 1)
variances <- moment(c(post_unif, post_beta), 2) - means^2

# cred. intervals are evaluated with quantile functions
creds <- list(
    unif = qbeta(thresh, alph_unif, beta_unif),
    beta = qbeta(thresh, alph_beta, beta_beta)
)

# redefine full names and colour names
prior_names <- c(unif = "Uniform", beta = "Beta(1, 4)")
prior_cols <- my_pal[1:2] |> set_names("unif", "beta")

# call the function that prints the table
summary_table(prior_names, means, variances, creds)
```
```{r}
# put data in long-format tibble, then plot
tibble(p = p, unif = post_unif(p), beta = post_beta(p)) |>
    pivot_longer(-1, names_to = "dist", values_to = "fp") |>
    mutate(dist = fct_relevel(dist, "unif", "beta")) |>
    ggplot(aes(x = p, y = fp, colour = dist)) +
        geom_line(linewidth = 0.8) +
        # shaded credibility intervals
        geom_ribbon(
            aes(ymin = 0, ymax = fp, fill = dist),
            data = function(x) {
                x |> group_by(dist) |>
                     filter(p > creds[[unique(dist)]][1] &
                            p < creds[[unique(dist)]][2])
            },
            alpha = 0.5,
            show.legend = FALSE
        ) +
        scale_colour_manual(values = prior_cols, labels = prior_names) +
        scale_fill_manual(values = prior_cols) +
        labs(x = "π", y = "g(π | y, n)", colour = "Prior") +
        coord_cartesian(expand = FALSE, ylim = c(0, 13.5)) +
        # fiddle with the axis breaks
        scale_y_continuous(breaks = scales::pretty_breaks())
```

Since we are working with quite a high number of samples, it’s sensible to
try approximating the two beta posteriors with normal PDFs.
```{r}
mu_unif <- means[1]
sigma_unif <- sqrt(variances[1])
mu_beta <- means[2]
sigma_beta <- sqrt(variances[2])

norm_unif <- function(p) dnorm(p, mu_unif, sigma_unif)
norm_beta <- function(p) dnorm(p, mu_beta, sigma_beta)
```

Let’s summarize the results in the usual table and plot.
```{r}
# credibility intervals
creds_n <- list(
    unif = list(
        orig = creds$unif,
        appr = qnorm(thresh, mu_unif, sigma_unif)
    ),
    beta = list(
        orig = creds$beta,
        appr = qnorm(thresh, mu_beta, sigma_beta)
    )
)

# summary table
# the means and variances are, by construction, the same as before
tibble(
    prior_names, means, variances, creds,
    creds_n = list(creds_n$unif$appr, creds_n$beta$appr)
) |>
    mutate(
        creds = format(creds, digits = 3),
        creds_n = format(creds_n, digits = 3)
    ) |>
    mutate_if(is.numeric, format, digits = 3) |>
    set_names(
        "Prior", "Mean", "Variance",
        "95% cred. int.", "95% cred. int. (normal appr.)"
    ) |>
    kableExtra::kbl(caption = "Posterior parameters") |>
    kableExtra::kable_styling(full_width = FALSE)
```
```{r}
# choose some new colours to distinguish original/approximation
type_cols <- c(orig = "#ac2941", appr = "#5aa4cd")
# make a new dataset with the new functions
tibble(
    p = p,
    unif = post_unif(p),
    unif_n = norm_unif(p),
    beta = post_beta(p),
    beta_n = norm_beta(p),
) |>
    pivot_longer(-1, names_to = "dist", values_to = "fp") |>
    # encode the original/approximation distinction in separate column
    mutate(
        type = ifelse(str_ends(dist, "_n"), "appr", "orig"),
        dist = str_remove(dist, "_n")
    ) |>
    # reorder factors as needed
    mutate(
        dist = fct_relevel(dist, "unif", "beta"),
        type = fct_relevel(type, "orig", "appr")
    ) |>
    ggplot(aes(x = p, y = fp, colour = type)) +
        geom_line(linewidth = 0.8) +
        geom_ribbon(
            aes(ymin = 0, ymax = fp, fill = type),
            data = function(x) {
                # group and filter
                x |>
                    group_by(dist, type) |>
                    filter(p > creds_n[[unique(dist)]][[unique(type)]][1] &
                           p < creds_n[[unique(dist)]][[unique(type)]][2])
            },
            alpha = 0.5,
            show.legend = FALSE
        ) +
        scale_colour_manual(
            values = type_cols, labels = c("Original", "Normal appr.")
        ) +
        scale_fill_manual(values = type_cols) +
        labs(x = "π", y = "g(π | y, n)", colour = "Type") +
        coord_cartesian(expand = FALSE, ylim = c(0, 13.5)) +
        scale_y_continuous(breaks = scales::pretty_breaks()) +
        facet_grid(
            rows = vars(dist),
            labeller = as_labeller(
                c(unif = "Uniform prior", beta = "Beta(1, 4) prior")
            )
        )
```

# Coin flipping
A coin is flipped 30 times with the following outcomes:
```
T, T, T, T, T, H, T, T, H, H,
T, T, H, H, H, T, H, T, H, T,
H, H, T, H, T, H, T, H, H, H
```

We’ll assume a uniform prior and a $\operatorname{Beta}(10, 10)$ prior, and
plot the likelihood, prior and posterior distributions following the two
assumptions.
```{r}
results <- c(
    "T", "T", "T", "T", "T", "H", "T", "T", "H", "H",
    "T", "T", "H", "H", "H", "T", "H", "T", "H", "T",
    "H", "H", "T", "H", "T", "H", "T", "H", "H", "H"
)

n <- length(results)     # total throws
h <- sum(results == "H") # number of heads

# likelihood
lhood <- function(p) {
    dbinom(h, n, p) / integrate(\(x) dbinom(h, n, x), 0, 1)$value
}

# beta prior parameters
a <- 10
b <- 10

# priors
prior_unif <- function(p) dunif(p, 0, 1)
prior_beta <- function(p) dbeta(p, a, b)

# posterior parameters
alph_unif <- 1 + h
beta_unif <- 1 + n - h
alph_beta <- a + h
beta_beta <- b + n - h

# posteriors
post_unif <- function(p) dbeta(p, alph_unif, beta_unif)
post_beta <- function(p) dbeta(p, alph_beta, beta_beta)
```

```{r}
# define palette
dist_cols <- my_pal |> set_names("lik", "pri", "pos")
dist_names <- c(lik = "Likelihood", pri = "Prior", pos = "Posterior")

tibble(
    p = p,
    # duplicate the likelihood to help facet_grid later
    unif_lik = lhood(p),
    unif_pri = prior_unif(p),
    unif_pos = post_unif(p),
    beta_lik = lhood(p),
    beta_pri = prior_beta(p),
    beta_pos = post_beta(p)
) |>
    pivot_longer(-1, names_to = "prior", values_to = "fp") |>
    # move lik/pri/pos indicator to `dist` column
    # keep only unif/beta indicator in `prior` column
    mutate(dist = str_sub(prior, -3), prior = str_sub(prior, 1, 4)) |>
    mutate(
        dist = fct_relevel(dist, "lik", "pri", "pos"),
        prior = fct_relevel(prior, "unif", "beta")
    ) |>
    ggplot(aes(x = p, y = fp, colour = dist)) +
        geom_line(linewidth = 0.8) +
        scale_colour_manual(values = dist_cols, labels = dist_names) +
        scale_fill_manual(values = dist_cols) +
        labs(x = "π", y = "f(π)", colour = "Distribution") +
        coord_cartesian(expand = FALSE, ylim = c(0, 6)) +
        facet_grid(
            rows = vars(prior),
            labeller = as_labeller(
                c(unif = "Uniform prior", beta = "Beta prior")
            )
        )
```

Let’s evaluate the most probable value for the coin probability $\pi$,
giving also estimates for the 95% credibility intervals.
```{r}
# most probable values for p
modes <- p[map_int(c(post_unif, post_beta), \(f) which.max(f(p)))]
# mean and variance
means <- moment(c(post_unif, post_beta), 1)
variances <- moment(c(post_unif, post_beta), 2) - means^2

# credibility intervals
creds <- list(
    unif = qbeta(thresh, alph_unif, beta_unif),
    beta = qbeta(thresh, alph_beta, beta_beta)
)

# summary table
tibble(names = c("Uniform", "Beta"), modes, means, variances, creds) |>
    mutate(creds = format(creds, digits = 3)) |>
    mutate_if(is.numeric, format, digits = 3) |>
    set_names("Prior", "Mode", "Mean", "Variance", "95% cred. int.") |>
    kableExtra::kbl(caption = "Posterior parameters") |>
    kableExtra::kable_styling(full_width = FALSE)
```

Now, we try to repeat the same analysis in a sequential way.  This means
that we’ll consider only one throw at each iteration, using the posterior
as prior in the next step.

```{r}
modes <- numeric(n)
creds <- replicate(n, numeric(2))
# define first prior parameters (uniform --> alpha = beta = 1)
a <- 1
b <- 1

for (i in seq_along(results)) {
    h <- results[i] == "H"
    a <- a + h
    b <- b + 1 - h

    post <- dbeta(p, a, b)
    modes[i] <- p[which.max(post)]
    # if mode = 0 compute the one-sided interval
    creds[, i] <- qbeta(if (modes[i] != 0) c(0.025, 0.975)
                        else c(0, 0.95), a, b)
}
```

Let’s plot the results.
```{r}
tibble(
    iter = seq_along(results),
    mode = modes,
    crmin = creds[1, ],
    crmax = creds[2, ]
) |>
    ggplot(aes(x = iter, y = mode)) +
        geom_line(aes(colour = "line"), linewidth = 0.8) +
        geom_ribbon(
            aes(ymin = crmin, ymax = crmax, fill = "ribbon"),
            alpha = 0.5
        ) +
        scale_colour_manual(values = "#4a417b", labels = "Mode") +
        scale_fill_manual(values = "#627bb4", labels = "95% cred. int.") +
        labs(x = "Number of tosses", y = "Posterior probability") +
        coord_cartesian(expand = FALSE, ylim = c(0, 1)) +
        scale_x_continuous(breaks = scales::pretty_breaks()) +
        theme(legend.title = element_blank())
```

As we can see, the end result is the same, in terms of the most probable
value. Let’s see if the credibility intervals are different.

```{r}
cat(
    "95% cred. int. from normal analysis:\n ",
    format(qbeta(thresh, alph_unif, beta_unif), digits = 3),
    " [uniform prior]\n ",
    format(qbeta(thresh, alph_beta, beta_beta), digits = 3),
    " [beta prior]\n\n95% cred. int. from sequential analysis\n ",
    format(creds[, ncol(creds)], digits = 3)
)
```

Indeed, the end result is identical to the one we had obtained from the
uniform prior in the normal analysis.

# Six boxes toy model 
We have six boxes labelled $H_i$ with $i = 0, 1, \dots, 5$. Each box $H_i$
contains $i$ white balls and $5 - i$ black balls. We randomly choose one
box without knowing which it is, and begin extracting balls from it,
putting them back after each extraction. Can we determine which box we have
based on the frequency of extracted white and black balls?

Let’s simulate the process with R.
```{r}
# probability of extracting a white ball from each box
p_white <- c(0, 0.2, 0.4, 0.6, 0.8, 1) |> set_names(1:6)
# boxes list
boxes <- map(0:5, \(i) c(rep("W", i), rep("B", 5 - i)))

# choose a random box
box_num <- sample.int(6, size = 1)
# sample balls with replacement from the chosen box
ntrials <- 30
balls <- sample(boxes[[box_num]], size = ntrials, replace = TRUE)
```

To construct a dataframe with the (posterior) probabilities associated with
each box, we start by filling the box columns with the probabilities of
extracting white or black from each box, depending on the values in the
`balls` vector. Then, we incrementally multiply every row, the likelihood,
with the previous, the prior. Finally, we normalize every row by its sum.
```{r}
probs <- map_dfr(p_white, \(p) ifelse(balls == "W", p, 1 - p)) |>
    map_dfr(\(col) accumulate(col, `*`, .init = 1 / 6)) |>
    mutate(rsum = rowSums(across(everything()))) |>
    mutate(across(-rsum, \(x) x / rsum), .keep = "none") |>
    add_column(trial = 0:ntrials, ball = c("-", balls), .before = 1)
```
```{r, echo = FALSE}
probs |>
    set_names("Trial", "Ball", map_chr(1:6, \(n) paste("Box", n))) |>
    kableExtra::kbl()
```

Finally, we plot the probability for each box as a function of the trial
number.
```{r, fig.width = 9, fig.height = 6}
box_labels <- c("●●●●●", "○●●●●", "○○●●●", "○○○●●", "○○○○●", "○○○○○") |>
    set_names(1:6)

probs |>
    select(-ball) |>
    pivot_longer(-trial, names_to = "box", values_to = "prob") |>
    ggplot(aes(x = trial, y = prob)) +
        geom_line(linewidth = 0.5, colour = my_pal[3]) +
        geom_point(size = 1.5, colour = my_pal[3]) +
        scale_y_continuous(
            limits = c(0, 1),
            breaks = scales::pretty_breaks()
        ) +
        labs(
            x = "Extraction",
            y = "Box probability",
            title = paste("Chosen box:", box_labels[box_num])
        ) +
        facet_wrap(vars(box), labeller = as_labeller(box_labels)) +
        theme(strip.background = element_rect(fill = "gray90")) +
        theme(strip.text = element_text(colour = "black"))
```